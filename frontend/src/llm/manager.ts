import { v4 as uuidv4 } from 'uuid';
// env ÏÑ§Ï†ï ÏóÜÏù¥ pipelineÎßå import
import { pipeline, FeatureExtractionPipeline } from '@xenova/transformers';

// Dynamic import types for node-llama-cpp
type Llama = any;
type LlamaModel = any;
type LlamaContext = any;
type LlamaChatSession = any;

export interface LLMManagerOptions {
  modelPath: string;
  // ADDED: ÏÇ¨Ïö©Ìï† ÏûÑÎ≤†Îî© Î™®Îç∏Ïùò Ïù¥Î¶ÑÏùÑ ÏòµÏÖòÏúºÎ°ú Ï∂îÍ∞ÄÌï† Ïàò ÏûàÏäµÎãàÎã§.
  embeddingModel?: string;
  onProgress?: (progress: number) => void;
  onEmbeddingProgress?: (status: string, progress?: number) => void;
}

export interface ChatOptions {
  temperature?: number;
  maxTokens?: number;
  topP?: number;
  systemPrompt?: string;
  sessionId?: string;
  onChunk?: (chunk: string) => void;
  onComplete?: () => void;
}

interface SessionData {
  id: string;
  context: LlamaContext;
  session: LlamaChatSession;
  systemPrompt?: string;
  createdAt: Date;
  messageCount: number;
  maxMessages: number;
}

export class LLMManager {
  private llama: Llama | null = null;
  private model: LlamaModel | null = null;
  private embeddingPipeline: FeatureExtractionPipeline | null = null;
  private sessions: Map<string, SessionData> = new Map();
  private options: LLMManagerOptions;
  private defaultSessionId: string = 'default';

  constructor(options: LLMManagerOptions) {
    this.options = options;
  }

  async initialize(): Promise<void> {
    try {
      console.log('Initializing llama.cpp...');

      // Dynamic import of node-llama-cpp (ESM module with top-level await)
      const { getLlama, LlamaChatSession: LlamaChatSessionClass } = await import('node-llama-cpp');

      // Store the session class for later use
      (this as any).LlamaChatSessionClass = LlamaChatSessionClass;

      this.llama = await getLlama();

      console.log('Loading model from:', this.options.modelPath);
      this.model = await this.llama.loadModel({
        modelPath: this.options.modelPath,
        onLoadProgress: (progress: number) => {
          if (this.options.onProgress) {
            this.options.onProgress(progress);
          }
          // console.log(`Model loading: ${(progress * 100).toFixed(1)}%`);
        }
      });

      console.log('Model loaded successfully');
      
      // ÏûÑÎ≤†Îî© Î™®Îç∏ Î°úÎìú
      await this.initializeEmbeddingModel();

      // Create default session
      await this.createSession();

    } catch (error) {
      console.error('Failed to initialize LLM:', error);
      throw error;
    }
  }

  private async initializeEmbeddingModel(): Promise<void> {
    // üí° ÏàòÏ†ï: Î¨∏Ï†úÏùò Î™®Îç∏ Ïù¥Î¶Ñ ÎåÄÏã†, Ìò∏ÌôòÏÑ±Ïù¥ Í≤ÄÏ¶ùÎêú ÏÉà Î™®Îç∏ Ïù¥Î¶ÑÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.
    const embeddingModelName = 'Xenova/all-MiniLM-L6-v2';
    
    try {
      if (this.options.onEmbeddingProgress) {
        this.options.onEmbeddingProgress('Initializing embedding model...', 0);
      }
      
      console.log('Loading embedding model:', embeddingModelName);
      
      // ‚úÖ Í∞ÄÏû• Í∞ÑÎã®Ìïú ÏõêÎûòÏùò ÏΩîÎìúÎ°ú Î≥µÍ∑Ä
      // Ïù¥ ÏΩîÎìú ÌïòÎÇòÎ°ú Îã§Ïö¥Î°úÎìúÎ∂ÄÌÑ∞ Î°úÎî©ÍπåÏßÄ Î™®Îëê ÏûêÎèôÏúºÎ°ú Ï≤òÎ¶¨Îê©ÎãàÎã§.
      this.embeddingPipeline = await pipeline(
        'feature-extraction', 
        embeddingModelName,
        {
          progress_callback: (progress: any) => {
            if (progress.status === 'progress' && progress.file) {
              const percent = (progress.loaded / progress.total) * 100;
              console.log(`Downloading ${progress.file}: ${percent.toFixed(1)}%`);
              
              if (this.options.onEmbeddingProgress) {
                this.options.onEmbeddingProgress(
                  `Downloading ${progress.file}...`,
                  percent
                );
              }
            } else if (progress.status === 'done') {
              console.log(`Downloaded: ${progress.file}`);
            }
          }
        }
      );
      
      if (this.options.onEmbeddingProgress) {
        this.options.onEmbeddingProgress('Embedding model ready', 100);
      }
      
      console.log('Embedding model loaded successfully');
      
    } catch (error) {
      console.error('Failed to load embedding model:', error);
      throw new Error(`Failed to initialize embedding model: ${error}`);
    }
  }

  async createSession(systemPrompt?: string): Promise<string> {
    if (!this.model) {
      throw new Error('Model not loaded');
    }

    const LlamaChatSessionClass = (this as any).LlamaChatSessionClass;
    if (!LlamaChatSessionClass) {
      throw new Error('LlamaChatSession class not loaded');
    }

    const sessionId = uuidv4();
    const context = await this.model.createContext({
      contextSize: 8192 // Gemma 3 context window
    });

    const session = new LlamaChatSessionClass({
      contextSequence: context.getSequence(),
      systemPrompt: systemPrompt || 'You are a helpful AI assistant.'
    });

    this.sessions.set(sessionId, {
      id: sessionId,
      context,
      session,
      systemPrompt,
      createdAt: new Date(),
      messageCount: 0,
      maxMessages: 100
    });

    // Set as default if it's the first session
    if (this.sessions.size === 1) {
      this.defaultSessionId = sessionId;
    }

    console.log(`Created session: ${sessionId}`);
    return sessionId;
  }

  async clearSession(sessionId: string): Promise<void> {
    const sessionData = this.sessions.get(sessionId);
    if (sessionData) {
      // Dispose context to free memory
      sessionData.context.dispose();
      this.sessions.delete(sessionId);
      console.log(`Cleared session: ${sessionId}`);
    }
  }

  async chat(message: string, options: ChatOptions = {}): Promise<string> {
    const sessionId = options.sessionId || this.defaultSessionId;
    const sessionData = this.sessions.get(sessionId);

    if (!sessionData) {
      throw new Error(`Session ${sessionId} not found`);
    }

    try {
      const response = await sessionData.session.prompt(message, {
        temperature: options.temperature ?? 0.7,
        maxTokens: options.maxTokens ?? 2048,
        topP: options.topP ?? 0.9,
      });

      sessionData.messageCount++;
      return response;
    } catch (error) {
      console.error('Chat error:', error);
      throw error;
    }
  }

  async streamChat(message: string, options: ChatOptions = {}): Promise<void> {
    const sessionId = options.sessionId || this.defaultSessionId;
    const sessionData = this.sessions.get(sessionId);

    if (!sessionData) {
      throw new Error(`Session ${sessionId} not found`);
    }

    try {
      await sessionData.session.prompt(message, {
        temperature: options.temperature ?? 0.7,
        maxTokens: options.maxTokens ?? 2048,
        topP: options.topP ?? 0.9,
        onTextChunk: (chunk: string) => {
          if (options.onChunk) {
            options.onChunk(chunk);
          }
        }
      });

      sessionData.messageCount++;

      if (options.onComplete) {
        options.onComplete();
      }
    } catch (error) {
      console.error('Stream chat error:', error);
      throw error;
    }
  }

    /**
   * Generates an embedding vector for the given text.
   * @param text The input text to create an embedding for.
   * @returns A promise that resolves to an array of numbers (the embedding vector).
   */
  async createEmbedding(text: string): Promise<number[]> {
    if (!this.embeddingPipeline) {
      throw new Error('Embedding model not loaded. Cannot create embedding.');
    }

    try {
      // ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Ïã§ÌñâÌïòÏó¨ ÌÖçÏä§Ìä∏Î°úÎ∂ÄÌÑ∞ ÏûÑÎ≤†Îî© ÌÖêÏÑúÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§.
      // pooling: 'mean' Í≥º normalize: true Îäî Î¨∏Ïû• ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ïãú ÌëúÏ§ÄÏ†ÅÏù∏ ÏòµÏÖòÏûÖÎãàÎã§.
      const embeddingTensor = await this.embeddingPipeline(text, {
        pooling: 'mean',
        normalize: true,
      });
      
      // ÌÖêÏÑú Îç∞Ïù¥ÌÑ∞Î•º ÏùºÎ∞ò JavaScript Î∞∞Ïó¥Î°ú Î≥ÄÌôòÌïòÏó¨ Î∞òÌôòÌï©ÎãàÎã§.
      const embedding = Array.from(embeddingTensor.data as Float32Array);
      // üí° ÏàòÏ†ï: slice(0, 10)ÏùÑ ÏÇ¨Ïö©Ìï¥ Î∞∞Ïó¥Ïùò Ï≤´ 10Í∞ú ÏöîÏÜåÎ•º Í∞ÄÏ†∏ÏôÄ Î°úÍ∑∏Ïóê Ï∂îÍ∞ÄÌï©ÎãàÎã§.
      // toFixed(4)Î°ú ÏÜåÏàòÏ†ê 4ÏûêÎ¶¨ÍπåÏßÄÎßå ÌëúÏãúÌïòÏó¨ Í∞ÄÎèÖÏÑ±ÏùÑ ÎÜíÏòÄÏäµÎãàÎã§.
      const preview = embedding.slice(0, 10).map(n => n.toFixed(4)).join(', ');
      console.log(`Generated embedding vector (dim: ${embedding.length}): [${preview}...]`);
      
      return embedding;
    } catch (error) {
      console.error('Failed to create embedding:', error);
      throw new Error('An error occurred while generating the embedding.');
    }
  }

  isEmbeddingModelReady(): boolean {
    return this.embeddingPipeline !== null;
  }

  getModelInfo() {
    if (!this.model) {
      return {
        name: 'Unknown',
        size: 0,
        quantization: 'Unknown',
        contextSize: 0,
        loaded: false,
        embeddingModelReady: false
      };
    }

    return {
      name: 'Gemma-3-12B-IT',
      size: 6_909_282_656, // ~6.4GB
      quantization: 'Q4_0',
      contextSize: 8192,
      loaded: true,
      embeddingModelReady: this.isEmbeddingModelReady()
    };
  }

  async cleanup(): Promise<void> {
    console.log('Cleaning up LLM Manager...');

    // Dispose all sessions
    for (const [sessionId, sessionData] of this.sessions) {
      sessionData.context.dispose();
    }
    this.sessions.clear();

    // Dispose model
    if (this.model) {
      this.model.dispose();
      this.model = null;
    }

    // Dispose embedding pipeline
    if (this.embeddingPipeline) {
      if (typeof (this.embeddingPipeline.model as any).dispose === 'function') {
        await (this.embeddingPipeline.model as any).dispose();
      }
      this.embeddingPipeline = null;
    }

    this.llama = null;
    console.log('LLM Manager cleanup complete');
  }

  // Cleanup old sessions to manage memory
  async cleanupOldSessions(maxAge: number = 3600000): Promise<void> {
    const now = Date.now();

    for (const [sessionId, sessionData] of this.sessions) {
      const age = now - sessionData.createdAt.getTime();
      if (age > maxAge && sessionId !== this.defaultSessionId) {
        await this.clearSession(sessionId);
      }
    }
  }

  // Get session count for monitoring
  getSessionCount(): number {
    return this.sessions.size;
  }

  // Get all session IDs
  getSessionIds(): string[] {
    return Array.from(this.sessions.keys());
  }
}
